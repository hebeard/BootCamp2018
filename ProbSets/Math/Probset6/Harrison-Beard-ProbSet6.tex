%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{color}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxlist}[1]
{\begin{list}{}
{\settowidth{\labelwidth}{#1}
 \setlength{\leftmargin}{\labelwidth}
 \addtolength{\leftmargin}{\labelsep}
 \renewcommand{\makelabel}[1]{##1\hfil}}}
{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\familydefault}{\sfdefault}

\usepackage{fancyhdr}
\pagestyle{fancy}


\usepackage{enumitem}
\setlist{nolistsep}
\usepackage{graphicx}



%\usepackage[proportional,scaled=1.064]{erewhon}
%\usepackage[erewhon,vvarbb,bigdelims]{newtxmath}
%\usepackage[T1]{fontenc}
%\renewcommand*\oldstylenums[1]{\textosf{#1}}

\usepackage{tikz}
 
\newcommand*\mycirc[1]{%
   \begin{tikzpicture}
     \node[draw,circle,inner sep=1pt] {#1};
   \end{tikzpicture}}


\usepackage{scalerel,stackengine}
\stackMath
\newcommand\hatt[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern.1pt\mathchar"0362\kern.1pt}%
    {\rule{0ex}{\textheight}}%WIDTH-LIMITED CIRCUMFLEX
  }{\textheight}% 
}{2.4ex}}%
\stackon[-6.9pt]{#1}{\tmpbox}%
}
\parskip 1ex





\stackMath
\newcommand\tildee[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern.1pt\mathchar"307E\kern.1pt}%
    {\rule{0ex}{\textheight}}%WIDTH-LIMITED CIRCUMFLEX
  }{\textheight}% 
}{2.4ex}}%
\stackon[-6.9pt]{#1}{\tmpbox}%
}
\parskip 1ex







\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\newcommand{\code}[1]{\texttt{#1}}





\usepackage{tcolorbox}
\tcbuselibrary{theorems}


\newtcbtheorem[]{kb}{Question}%
{colback=white,colframe=blue!65!black,fonttitle=\bfseries}{th}




\usepackage{lastpage}





\lhead{Harrison Beard}

\rhead{\textsf{OSM Lab Boot Camp}\textsf{\textbf{ Math Problem Set 6}}}


\cfoot{Page  \thepage /\pageref{LastPage}}

\AtBeginDocument{
  \def\labelitemiii{ }
  \def\labelitemiv{ }
}

\makeatother

\usepackage{babel}
\begin{document}
\global\long\def\n#1{\left\Vert #1\right\Vert }
\global\long\def\eval#1{\left.#1\right|}
\global\long\def\R{\mathbb{R}}
\global\long\def\N{\mathbb{N}}
\global\long\def\Quo{\mathbb{Q}}
\global\long\def\F{\mathbb{F}}
\global\long\def\cm{^{\complement}}
\global\long\def\pow#1{\mathcal{P}\left(#1\right)}
\global\long\def\es{\mbox{\ensuremath{\emptyset}}}
\global\long\def\pr{^{\prime}}
\global\long\def\Com{\mathbb{C}}
\global\long\def\part#1#2{\frac{\partial#1}{\partial#2}}
\global\long\def\sm{\smallsetminus}
\global\long\def\usub#1#2#3#4{\underset{\phantom{#3}#2\phantom{#4}}{#3\underbrace{#1}#4}}
\global\long\def\E#1{\mathrm{E}\left[#1\right]}
\global\long\def\Var#1{\mathrm{Var}\left[#1\right]}
\global\long\def\li#1#2{\int_{#2}#1\,\mathrm{d}\mu}
\global\long\def\e#1{\mathrm{e}^{#1}}
\global\long\def\G#1{\Gamma\left(#1\right)}
\global\long\def\ep{\varepsilon}
\global\long\def\P{\mathrm{P} }
\global\long\def\CS#1#2{\left\{  \left.#1\phantom{\mathllap{#2}}\right|#2\right\}  }
\global\long\def\inn#1#2{\left\langle #1,#2\right\rangle }
\global\long\def\span#1{\mathrm{span}\left\{  #1\right\}  }
\global\long\def\H{^{\mathrm{\mathsf{H}}}}
\global\long\def\T{^{\mathsf{T}}}
\global\long\def\dd{\mathbf{d}}
\global\long\def\tr#1{\mathrm{tr}\left(#1\right)}
\global\long\def\proj#1#2{\mathrm{proj}_{#1}\left(#2\right)}
\global\long\def\d{\mathrm{d}}
\global\long\def\qed{\ \hfill{\color{blue}\blacksquare}}
\global\long\def\i#1#2{\int#1\,\mathrm{d}#2}
\global\long\def\diff#1#2{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\global\long\def\nb#1#2{\left\Vert #1\right\Vert _{#2}}
\global\long\def\Fs{\mathrm{F}}
\global\long\def\iid{\stackrel{\mbox{iid}}{\sim}}
\global\long\def\L{\mathscr{L}}
\global\long\def\Norm#1#2{\mathcal{N}\left(#1,#2\right)}
\global\long\def\s{^{\ast}}
\global\long\def\im{\mathrm{im}}
\global\long\def\Skew#1#2{\mathrm{Skew}_{#1}\left(#2\right)}
\global\long\def\rank#1{\mathrm{rank}\left(#1\right)}
\global\long\def\Sym#1#2{\mathrm{Sym}_{#1}\left(#2\right)}
\global\long\def\v{\mathbf{v}}
\global\long\def\basis#1{\mathrm{basis}\left(#1\right)}
\global\long\def\l#1{\left(\,\textit{#1}\,\right).}
\global\long\def\conv#1{\mathrm{conv}\left(#1\right)}
\global\long\def\x{\mathbf{x}}
\global\long\def\lcr#1#2#3{#1\hfill#2\hfill#3}
\global\long\def\D{\mathbf{D}}
\global\long\def\A{\mathbf{A}}
\global\long\def\Q{\mathbf{Q}}
\global\long\def\B{\mathbf{B}}
\global\long\def\ppr{^{\prime\prime}}
\global\long\def\pppr{^{\prime\prime\prime}}
\global\long\def\ppppr{^{\imath v}}
\global\long\def\u{\mathbf{u}}
\global\long\def\y{\mathbf{y}}
\global\long\def\p{\mathbf{p}}
\global\long\def\z{\mathbf{z}}
\global\long\def\o{\mathbf{0}}
\global\long\def\a{\mathbf{a}}
\global\long\def\b{\mathbf{b}}
\global\long\def\t{\T}
\global\long\def\I{\mathbf{I}}
\global\long\def\h{\H}
\global\long\def\r{\R}
\global\long\def\M#1#2{\mathrm{M}_{#1}\left(#2\right)}
\global\long\def\gmm#1{\hat{#1}_{\mathrm{GMM}}}
\global\long\def\mle#1{\hat{#1}_{\mathrm{MLE}}}
\global\long\def\inv{^{-1}}
\global\long\def\nn{\inv}
\global\long\def\lik#1#2{\mathcal{L}\left(#1\mid#2\right)}
\global\long\def\cs#1#2{\left(#1\mid#2\right)}
\global\long\def\W{\mathbf{W}}
\global\long\def\th{\boldsymbol{\theta}}
\global\long\def\smm#1{\hat{#1}_{\mathrm{SMM}}}
\global\long\def\C{\mathbf{C}}
\global\long\def\Unif#1#2{\mathrm{Unif}\left(#1,#2\right)}
\global\long\def\thm{{\color{cyan}\vartriangleright\mbox{ \textbf{Thm. }}}}
\global\long\def\defn{{\color{red}\triangle\mbox{ \textbf{Def. }}}}
\global\long\def\ex{\mbox{\ensuremath{\lozenge}\ \textbf{Example. }}}
\global\long\def\note{\mycirc{!}\,\textbf{Note.}\,}
\global\long\def\lemm{{\color{cyan}\vartriangleright\mbox{ \textbf{Lemma. }}}}
\global\long\def\coro{{\color{cyan}\vartriangleright\mbox{ \textbf{Cor. }}}}
\global\long\def\pf{\square\,\textbf{Proof.}\,}
\global\long\def\soln{{\color{blue}\,\textbf{Solution.}\,}}
\global\long\def\c{\mathbf{c}}
\global\long\def\kw#1{\textbf{{\color{blue}#1}}\index{#1}}
\global\long\def\kww#1{\textbf{{\color{white}#1}}\index{#1}}
\global\long\def\break{\smallskip{}}
\global\long\def\bbreak{\bigskip{}\bigskip{}}
\global\long\def\endex{\;\hfill\blacklozenge}
\global\long\def\line{\rule[0.5ex]{1\columnwidth}{1pt}}
\global\long\def\npg{\newpage{}}
\global\long\def\endday{\begin{array}{c}
 \ \hfill\mbox{\textbf{end of day.}}\\
 \line
\end{array}}
\global\long\def\ss{\mathbf{s}}


\global\long\def\begday#1#2#3#4{\begin{array}{c}
 \begin{array}{c}
 \resizebox{5cm}{!}{\textbf{#1, #2 #3. #4}}\qquad\qquad\qquad\,\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\;\;\;\;\;\;\;\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\end{array} \end{array}\line}



\title{\textsf{OSM Lab Boot Camp}\textsf{\textbf{ Math Problem Set 6}}}


\author{\textsf{Harrison Beard}}


\date{\textsf{30 July 2018}}

\maketitle
\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.1.}]

Prove that an unconstrained linear objective function is either constant or has no minimum.

\end{tcolorbox}

$\soln$Proof by contradiction. If an objective function $f$ is \emph{not}
constant, then we know there must be a $\tilde{\x}$ such that $f\left(\tilde{\x}\right)\neq f\left(\x\s\right)$
for the $\x\s$ that minimizes $f$, so $f\left(\tilde{\x}\right)$
cannot be less than $f\left(\x\s\right)$ without loss of generality.
Then, this implies $f\left(\tilde{\x}\right)>f\left(\x\s\right)$,
so it follows that $f\left(\x\s-\tilde{\x}\right)<0$ by linearity,
but then we have 
\begin{eqnarray*}
f\left(2\x\s-\tilde{\x}\right) & = & f\left(\x\s\right)+f\left(\x\s-\tilde{\x}\right)\\
 & < & f\left(\x\s\right)\text{,}
\end{eqnarray*}
which is a contradiction because $\x\s$ is the minimizer by assumption.
$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.2.}]

Prove that if $\b\in\R^m$ and $\A\in\M{m\times n}{\R}$, then the problem of finding an $\x\s\in\R^n$ to minimize $\n {\A\x-\b} _2$ is equivalent to minimizing\\
\\
\\
\\

$\lcr{}{\x\t\A\t\A\x-2\b\t\A\x.}{(9.21)}$\\
\\
\\
\\

In Voume 1, Chapter 3 we use projections to prove that this is equivalent to solving the normal equation

\[
\A\t\A\x=\A\t\b.
\]

Use the first- and second-order conditions to give a different proof that minimizing $(9.21)$ is equivalent to solving the normal equation.


\end{tcolorbox}

$\soln$We will use the fact that $\A\t\A$ is symmetric and \emph{positive
definite }as we have shown previously. 

Note that if $\x$ is such that the FONC is satisfied, then $\x$
is a global minimizer of the function 
\[
\left(\A\x-\b\right)\t\left(\A\x-\b\right)=\x\t\A\t\A\x-2\b\t\A\x\text{.}
\]
Next, note that our FOC is $2\A\t\A\x-2\A\t\b=0$, so we have that
$\A\t\A\x=\A\t\b$, and by the SOC and positive definiteness of $\A\t\A$,
so $2\A\t\A>0$, and so we are done.$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.3.}]

For each of the multivariable optimization methods we have discussed in this section, list the following:

\begin{lyxlist}{00.00.0000} 

\item [{$\l i$}] The basic idea of the method, including how it differs from the other methods in the list. Include any geometric description you can give of the method.

\item[{$\l {ii}$}] What types of optimization problems it can solve and cannot solve.

\item [{$\l {iii}$}] Relative strengths of the method.

\item [{$\l {iv}$}] Relative weaknesses of the method.

\end{lyxlist}

\end{tcolorbox}

$\soln$
\begin{lyxlist}{00.00.0000}
\item [{$\l i$}] The method of \textbf{Gradient Descent} is to follow
the function $f$'s negative gradient $-\D f\t\left(\x_{i}\right)$
and iterate over the $\x_{i}$'s, recalculating the gradient, until
a minimum is reached. Geometrically, this looks like a zig-zag pattern
across the level sets of $f$. \textbf{Newton's Method} is used when
the dimensionality is small and if the Hessian $\D^{2}f\left(\x_{i}\right)$
is positive definite; its appeal is that it converges quadratically
and acts as both a local approximation and a descent method. \textbf{Conjugate
Gradient} can optimize a quadratic in a single step, and is very quick
for small-dimensional problems. It is usually contrasted with gradient
descent geometrically, as it appears as a straight line perpendicular
to each of the level sets of $f$ by moving along $\Q$-conjugate
directions, as seen in figure $(9.1)$ in the textbook. \textbf{Gauss-Newton}
is an adaptation of Newton's method to efficiently optimize nonlinear
least squares (NLS) problems. One of the quasi-Newton innovations,
\textbf{Broyden-Fletcher-Goldfarb-Shanno (BFGS)}, was developed to
improve computational cost involves only computing the Hessian initially,
but has a lower convergence rate.
\item [{$\l{ii}$}] \textbf{Conjugate Gradient} is used when $f$ is differentiable.
\textbf{Newton's Method} is used when $\D^{2}f\left(\x_{i}\right)\in\mathrm{PD}_{n}\left(\R\right)$.
\textbf{Gauss-Newton} and its derivatives are used for NLS problems.
\item [{$\l{iii}$}] \textbf{Newton} has quadratic convergence and is best
when the dimension of the problem is not large and when $\x_{0}\approx\x\s$.
\textbf{Gauss-Newton} is best for NLS problems. The niche appeal to
both Gauss-Newton and \textbf{BFGS} is that they both are effective
when $\left(\D^{2}f\left(\x\right)\right)^{-1}\D f\left(\x\right)$
is expensive or error-prone. When $\x_{0}$ is far from $\x\s$, then
\textbf{Gradient Descent }is the fastest algorithm and most desirable
if the dimensionality is reasonable. \textbf{Conjugate Gradient }is
best for solving large quadratic linear systems when $f=\frac{1}{2}\x\t\Q\x-\b\t\x+c$
and $\Q$ is symmetric, positive definite, and sparse. 
\item [{$\l{iv}$}] \textbf{Newton }and its derivatives becomes prohibitively
expensive when the dimension of the problem is large or when $\x_{0}$
starts very far away from $\x\s$. \textbf{BFGS} fails under similar
criteria. \textbf{Conjugate Gradient} loses its appeal when the number
of nonzero entries $m$ of $\Q$ becomes very large, since its temporal
and spatial complexity ${\cal O}\left(m\right)$ is often contrasted
with Newton's single-iteration complexity ${\cal O}\left(n^{3}\right)$.
\end{lyxlist}
$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.4.}]

Let $f(\x) = \frac {1} {2} \x\t\Q\x - \b\t\x$, where $\Q\in\M{m\times n}{\R}$ satisfies $\Q>0$ and $\b\in\R^n$. Show that the Method of Steepest Descent (that is, gradient descent with optimal line search), converges in one step (that is, $\x_1=\Q^{-1}\b$), if and only if $\x_0$ is chosen such that $\D f\left(\x_0\right)\t=\Q\x_0-\b$ is an eigenvector of $\Q$ (and $\alpha_0$ satisfies $(9.2)$).

\end{tcolorbox}

$\soln$For $\D f(\x_{0})\t=\Q\x_{0}-\b$ with corresponding eigenvalue
$\lambda_{\Q}$, we have
\begin{eqnarray*}
\x_{1} & = & \x_{0}-\frac{\D f(\x)\t\D f(\x)}{\D f(\x)\t\Q\D f(\x)}\D f(\x)\\
 & = & \x_{0}-\frac{\D f(\x)\t\D f(\x)}{\D f(\x)\t\lambda_{\Q}\D f(\x)}\D f(\x)\\
 & = & \x_{0}-\frac{1}{\lambda}\D f(\x)\\
 & = & \x_{0}-\Q^{-1}\D f(\x)\\
 & = & \x_{0}-\Q^{-1}\left(\Q\x_{0}-\b\right)\\
 & = & \Q^{-1}\b\text{.}
\end{eqnarray*}
$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.5.}]

Assume that $f:\R^n\to\R$ is $\mathcal{C}^1$. Let $\left\{ \x_k \right\} _{k=0}^\infty$ be defined by the Method of Steepest Descent. Show that if $\x_{k+1} - \x_k$ is orthogonal to $\x_{k+2}-\x_{k+1}$ for each $k$.

\end{tcolorbox}

$\soln$We have
\begin{eqnarray*}
\diff{f\left(\x_{k+1}\right)}{\alpha_{k}} & = & \D f\left(\x_{k+1}\right)\t\D f\left(\x_{k}\right)\\
 & = & 0\text{,}
\end{eqnarray*}
and since $f\left(\x_{k+1}\right)\coloneqq f\left(\x_{k}+\alpha_{k}\D f\left(\x_{k}\right)\t\right)$,
we have that
\[
\x_{k+1}-\x_{k}=-\alpha_{k}\D f\left(\x_{k}\right)\t\text{,}
\]
where $\x_{k+1}-\x_{k}$ and $\x_{k+2}-\x_{k+1}$ are orthogonal by
assumption. $\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.6.}]

Write a Python/NumPy routine for implementing the steepest descent method for quadratic functions (see Example $9.2.3$).\\

Given a small number $\ep$, given Numpy arrays $\x_0, \b$ of length $n$, and given an $n\times n$ matrix $\Q>0$, your code should return a close approximation to a local minimizer $\x\s$ of $f=\x\t\Q\x - \b\t\x + c$.\\

For the stopping criterion, use the condition $\n {\D f \left( \x _k \right)}$ for some small value of $\ep$.

\end{tcolorbox}

$\soln$

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

def p9_6(Q,b,x_0,epsilon=1e-8,K=500):
	"""
	Steepest Descent.
	"""
	
	norm,k=1,1
	while (k<K) and (norm>epsilon): # stopping criteria

		# calculate Df
		Df = Q @ x_0 - b
		norm = np.linalg.norm(Df)
		alpha = Df Df.T @ Df / (Df.T @ Q @ Df)
	
		# update x sequence
		x_1 = x_0 - alpha * Df
		x_0 = x_1
	
	if k<K: print("Converged!")

	print("\nx_0:\n",x_0)
\end{lstlisting}

$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.7.}]

Write a simple Python/NumPy method for computing $\D f$ using forward differences and a step size of $\sqrt {\mathrm{Rerr}_f}$. It should accept a callable function $f:\R^n\to\R$, a point $\x\in\R^n$, and an estimate $\mathrm{Rerr}_f>\ep$ for the maximum relative error of $f$ near $\x$. It should return an estimate for $\D f (\x)$.

\end{tcolorbox}

$\soln$

\begin{lstlisting}
def p9_7(f,x_0,rerr):
	"""
	Computing Df with forward differences.
	"""
	
	# set dims
	m,n = f(x_0).shape[0],x_0.shape[0]
	if len(m)==0: m=1
	
	Df = np.zeros((m,n)) # initialize
	h = 2*np.sqrt(rerr)

	for i in range(n):
		unit_vec = np.zeros(n)
		unit_vec[i] = 1
		Df[:,i] = (f(x_0+h*unit_vec)-f(x_0)) / h
	
	return Df
\end{lstlisting}

$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.8.}]

Use your differentiation method from the previous problem to construct a simple Python/NumPy method for implementing the steepest descent method for arbitrary functions, using the secant method (Exercise $6.15$) for the line search.\\

Your method should accept a callable function $f$, a starting value $\x_0$, a small number $\ep$, a NumPy array $\x_0$ of length $n$, and return a close approximation to a local minimizer $\x\s$ of $f$.\\

For the stopping criterion, use the condition $\n {\D f \left( \x _k \right)}$.

\end{tcolorbox}

$\soln$

\begin{lstlisting}
import matplotlib.pyplot as plt 
import numpy as np from scipy 
import linalg as la

# Globals 
MAX_IT = 10_000 
RERR = 1+1e-10 
# X = np.linspace(-10,10,1_000)
TOL = 1e-10  
STARTING_VALS = [1,.5]  

def Df(f,x,rerr):
	Df=np.zeros(len(x))
	for i in range(len(x)):
		Df[i]= ((-3*f(x)+4*f(x+2*np.sqrt(rerr)*np.eye(len(x))[:,i])-f(x+4*np.sqrt(rerr)*np.eye(len(x))[:,i]))/
			(4*np.sqrt(rerr)))
	return Df

def secant_method(x_0, x_1, ep, f):          	f_pr = p9_7(f,x_0,RERR)      
	k = 0    
	x_k = x_0   
	x_kp1 = x_1     
	while k < MAX_IT:         
		x_km1 = x_k         
		x_k = x_kp1         
		x_kp1 = x_k - f_pr(x_k) * (x_k - x_km1)/(f_pr(x_k) - f_pr(x_km1))         
		k += 1 
#         plt.plot(X, f(X), "b--") 
#         plt.plot(x_k, f(x_k), "ko") 
#         plt.show()         
	if la.norm(x_kp1 - x_k) < ep * la.norm(x_k):             
		break     
	return x_k

def steepest_descent(f,x_vec,ep):
	x_k,k = x_vec,1
	norm = 1 + ep
	while (k<MAX_IT) and (norm>ep)
		Df_k=Df(f,x_k,ep)
		f_a = lambda x: Df(f,x_k-x_vec*Df_k.T,ep) @ -Df_k.T
		alph = secant_method(STARTING_VALS[0],STARTING_VALS[1],ep,f_a)
		x_kp1 = x_k - alph*Df_k
		norm = la.norm(Df_k)
		x_k = x_kp1
		k+=1
	return x_k
\end{lstlisting}

$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.9.}]

Apply your code from the previous problem to the Rosenbrock function
\[
f(x,y) = 100 \left( y-x^2 \right) ^2 + (1-x)^2
\]
with an initial guess of $\left(x_0,y_0\right)=(-2,2)$.

\end{tcolorbox}

$\soln$

\begin{lstlisting}
def F(x_vec):
	""" rosenbrock function """
	return 100*(x_vec[1]-x_vec[0]**2)**2+(1-x_vec[0])**2
def p9_9():
	x_s = steepest_descent(F,np.array([-2,2]),TOL)
	print("optimum at",x_s,"with value of",F(x_s))
\end{lstlisting}

The optimum I found through the function I wrote above was at $\x\s=\begin{pmatrix}1\\
1
\end{pmatrix}$ with value $0$.$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.10.}]

Consider the quadratic function $f(\x)=\frac 1 2 \x\t\Q\x-\b\t\x$, where $\Q\in\M n \R$ is symmetric and positive definite and $\b\in\R^n$. Show that for any initial guess $\x_0\in\R^n$, one iteration of Newton's method lands at the unique minimizer of $f$.

\end{tcolorbox}

$\soln$We know that the FOC is that $\D f=0$, so since $\D f=\Q\x-\b$
and $\D^{2}f=\Q$, then we know that for any $\x_{0}$, we have 
\begin{eqnarray*}
\x_{1} & = & \x_{0}-\Q^{-1}\D f\left(\x_{0}\right)\\
 & = & \x_{0}-\Q^{-1}\left(\Q\x_{0}-\b\right)\\
 & = & \Q^{-1}\b\text{,}
\end{eqnarray*}
so $\x_{1}$ must optimize $f$.$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.12.}]

Prove that if $\A\in\M n \F$ has eigenvalues $\lambda_1,\ldots,\lambda_n$ and $\B=\A+\mu\I$, then the eigenvectors of $\A$ and $\B$ are the same, and the eigenvalues of $\B$ are $\mu+\lambda_1,\mu+\lambda_2,\ldots,\mu+\lambda_n$.

\end{tcolorbox}

$\soln$Note that
\[
\A\x=\lambda\x,
\]
where $\A=\B-\mu\I$ so we have
\[
\B\x=\left(\lambda+\mu\right)\x
\]
through some algebra.$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.15.}]

Prove the Sherman-Morrison-Woodbury formula $(9.13)$.\\

$(9.13).$ \textit{Let $\A$ be a nonsingular $n\times n$ matrix, $\B$ an $n\times\ell$ matrix, $\C$ a nonsingular $\ell\times\ell$ matrix, and $\D$ an $\ell\times n$ matrix. We have}\\
\\
\\
\\

$\lcr{}{      (\A+\B\C\D)\inv=\A\inv-\A\inv\B\left(\C\inv+\D\A\inv\B\right)\inv\D\A\inv.    }{(9.13)}$\\
\\
\\
\\

\end{tcolorbox}

$\soln$Left-multiply the RHS by the LHS, and we get the identity:
\begin{eqnarray*}
 &  & \left(\A+\B\C\D\right)\left(\A^{-1}-\A^{-1}\B\left(\C^{-1}+\D\A^{-1}\C\right)^{-1}\D\A^{-1}\right)\\
 & = & \I+\B\C\D\A^{-1}-\left(\B+\B\C\D\A^{-1}\B\right)\left(\C^{-1}+\D\A^{-1}\B\right)^{-1}\D\A^{-1}\\
 & = & \I+\B\C\D\A^{-1}-\B\C\left(\C^{-1}+\D\A^{-1}\B\right)\left(\C^{-1}+\D\A^{-1}\B\right)^{-1}\D\A^{-1}\\
 & = & \I+\B\C\D\A^{-1}-\B\C\D\A^{-1}\\
 & = & \I.
\end{eqnarray*}
$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.16.}]

Use $(9.13)$ to derive $(9.14)$.\\
\\
\\
\\

$\lcr{}{\A_k\inv=\A_{k-1}\inv+\frac{\left(\ss_{k-1}-\A\inv_{k-1}\y_{k-1}\right)\ss\t_{k-1}\A\inv_{k-1}}{\ss\t_{k-1}\A\inv_{k-1}\y_{k-1}}.}{(9.14)}$\\
\\
\\
\\

\end{tcolorbox}

$\soln$We start with the Broyden's Method updating criteria
\[
\A_{k+1}=\A_{k}+\frac{\y_{k}-\A_{k}\ss_{k}}{\n{\ss_{k}}^{2}}\ss_{k}\t
\]
and recognize the following matrices: 
\[
\usub{\A_{k+1}=\usub{\A_{k}}{n\times n}{}{}+\usub{\frac{\overset{n\times\ell}{\overbrace{\y_{k}}}-\overset{n\times n}{\overbrace{\A_{k}}}\overset{n\times\ell}{\overbrace{\ss_{k}}}}{\n{\ss_{k}}^{2}}}{\B}{}{}\usub 1{\C}{\cdot}{\cdot}\usub{\overset{\ell\times n}{\overbrace{\ss_{k}\t}}}{\D}{}{\text{.}}}{(\ast)}{}{}
\]
From here, we apply the Sherman-Morrison-Woodbury formula, 
\[
(\A+\B\C\D)\inv=\A\inv-\A\inv\B\left(\C\inv+\D\A\inv\B\right)\inv\D\A\inv\text{,}
\]
to equation $(\ast)$ to yield
\[
\A_{k}\inv=\A_{k-1}\inv+\frac{\left(\ss_{k-1}-\A\inv_{k-1}\y_{k-1}\right)\ss\t_{k-1}\A\inv_{k-1}}{\ss\t_{k-1}\A\inv_{k-1}\y_{k-1}}\text{,}
\]
which is of the form $(9.14)$.$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.18.}]

Let $\Q\in\M n{\R}$ satisfy $\Q>0$, and let $f$ be the quadratic function $f\left(\x\right)=\frac{1}{2}\x\t\Q\x-\b\t\x+c$. Given a starting point $\x_{0}$ and $\Q$-conjugate directions $\mathbf{d}_{0},\mathbf{d}_{1},\ldots,\mathbf{d}_{n-1}$ in $\R^n$, show that the optimal line search solution for $\x_{k-1}=\x_{k}+\alpha_{k}\mathbf{d}_{k}$ (that is, the $\alpha$ which minimizes $\phi_{k}(\alpha)=f\left(\x_{k}+\alpha_{k}\mathbf{d}_{k}\right))$ is given by $\alpha_{k}=\frac{\mathbf{r}\t_{k}\mathbf{d}_{k}}{\mathbf{d}_{k}\t\Q\mathbf{d}_{k}}$, where $\mathbf{r}_{k}=\b-\Q\x_{k}$. 

\end{tcolorbox}

$\soln$For $\Q>0$, we have that $\dd_{k}\t\Q\dd_{k}>0$ by definition,
so we can find a minimizer $\alpha_{k}$ of $\phi_{k}\left(\alpha_{k}\right)$
for $\phi_{k}\left(\alpha_{k}\right)$ defined as follows. 
\begin{eqnarray*}
\phi_{k}\left(\alpha_{k}\right) & = & f\left(\x_{k}+\alpha_{k}\dd_{k}\right)\\
 & = & \frac{1}{2}\x_{k}\t\Q\x_{k}+\alpha_{k}\dd_{k}\t\Q\x_{k}\\
 &  & +\frac{1}{2}\alpha_{k}^{2}\dd_{k}\t\Q\dd_{k}-\x_{k}\t\b-\alpha_{k}\dd_{k}\t\b\text{.}
\end{eqnarray*}
Taking FOCs,
\begin{eqnarray*}
0 & = & \part{\phi_{k}\left(\alpha_{k}\right)}{\alpha_{k}}\\
 & = & \alpha_{k}\dd_{k}\t\Q\dd_{k}-\dd_{k}\t\b+\dd_{k}\t\Q\x_{k}
\end{eqnarray*}
so 
\begin{eqnarray*}
\alpha_{k}\dd_{k}\t\Q\dd_{k} & = & \dd_{k}\t\b+\dd_{k}\t\Q\x_{k}\\
 & = & \dd_{k}\t\left(\b-\Q\x_{k}\right)\\
 & = & \dd_{k}\t\mathbf{r}_{k}\text{,}
\end{eqnarray*}
so
\[
\alpha_{k}=\dd_{k}\t\mathbf{r}_{k}\left(\dd_{k}\t\Q\dd_{k}\right)^{-1}\text{.}
\]
$\qed$
\[
\break
\]


\begin{tcolorbox}
[colback=blue!10!white,colframe=blue!65!black,fonttitle=\bfseries,title=\kww{Exercise 9.20.}]

Prove Lemma 9.5.5.\\

\textit{\textbf{Lemma 9.5.5.} In the Conjugate Gradient Algorithm, $\mathbf{r}\t_{i}\mathbf{r}_{k}=0$ for all $i<k$.}

\end{tcolorbox}

$\soln$Let ${\cal B}_{i}=\left\{ \b-\Q\x_{k}\right\} _{k=0}^{i-1}$
be our basis. Then, through Gram-Schmidt, we would have
\[
\mathbf{r}_{k}=\b-\Q\x_{k}-\sum_{i=0}^{k-1}\frac{\inn{\mathbf{r}_{i}}{\b-\Q\x_{k}}}{\n{\mathbf{r}_{i}}^{2}}\mathbf{r}_{i}\text{;}
\]
here we see that each $\mathbf{r}_{i}$ is a linear combination of
some elements in $\left\{ \b-\Q\x_{k}\right\} _{k=0}^{i-1}$, so then
it follows that ${\cal B}_{i}=\mathrm{span}\left(\left\{ \mathbf{r}_{k}\right\} _{k=0}^{i-1}\right)$.
So, our conjugate gradient problem becomes
\[
\min_{\x\in\x_{0}+{\cal B}_{i}}f\left(\x\right)\text{,}
\]
where $\x\s=\x_{i}$, so we have that $h=0$ minimizes
\[
\min_{h}f\left(\x_{i}+h\left(\b-\Q\x_{j}\right)\right)
\]
for all $i>j$. So, our FOC is
\begin{eqnarray*}
0 & = & \D f\left(\x_{i}\right)\left(\b-\Q\x_{j}\right)\\
 & = & \left(\Q\x_{i}-\b\right)\t\left(\b-\Q\x_{j}\right)\\
 & = & -\left(\b-\Q\x_{i}\right)\t\left(\b-\Q\x_{j}\right)\text{.}
\end{eqnarray*}
$\qed$
\end{document}
